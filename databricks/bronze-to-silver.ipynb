{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6035a88c-8f0f-4c61-a66b-b2add072b555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rename columns and save data frames to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a80093-b009-4f28-b25f-079781ded133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def camel_case_to_snake_case(column_name):\n",
    "    # Add an underscore before any uppercase letter followed by a lowercase letter\n",
    "    # and convert the entire string to lowercase\n",
    "    return re.sub(r'(?<!^)(?=[A-Z])', '_', column_name).lower()\n",
    "\n",
    "\n",
    "tables_dict = {\n",
    "    'reservations': None,\n",
    "    'flights': None,\n",
    "    'customers': None,\n",
    "    'hotels': None,\n",
    "    'eur_exchange_rates': None,\n",
    "}\n",
    "\n",
    "for table_name in tables_dict.keys():\n",
    "    df = spark.read.parquet(\n",
    "        f'{BRONZE_LAYER_PATH}/{table_name}'\n",
    "    )\n",
    "    \n",
    "    # Rename data frame columns\n",
    "    renamed_columns = [camel_case_to_snake_case(column) for column in df.columns]\n",
    "    df = df.toDF(*renamed_columns)\n",
    "\n",
    "    # Save data frame to dictionary\n",
    "    tables_dict[table_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74769d55-352f-453d-8aa2-ee99184cd06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reservations Data Frame transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54215cf3-710b-4503-bbb0-226a69aaa745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, regexp_extract, to_timestamp, when, lit, round\n",
    "\n",
    "\n",
    "reservations_df = tables_dict['reservations']\n",
    "\n",
    "# Convert the reservation_created_at column to timestamp\n",
    "reservations_df = reservations_df.withColumn(\n",
    "    'reservation_created_at',\n",
    "     to_timestamp('reservation_created_at')\n",
    ")\n",
    "\n",
    "# Create columns for date and time separately\n",
    "reservations_df = reservations_df.withColumn(\n",
    "    'reservation_date',\n",
    "    date_format('reservation_created_at', 'yyyy-MM-dd')\n",
    ")\n",
    "reservations_df = reservations_df.withColumn(\n",
    "    'reservation_time',\n",
    "    date_format('reservation_created_at', 'HH:mm')\n",
    ")\n",
    "\n",
    "# Extract the price amount and round it to two decimal places\n",
    "reservations_df = reservations_df.withColumn(\n",
    "    'total_price_amount',\n",
    "    round(regexp_extract(col('total_price'), r'(\\d+\\.\\d+)', 1).cast('float'), 2)\n",
    ")\n",
    "\n",
    "# Extract the price currency\n",
    "reservations_df = reservations_df.withColumn(\n",
    "    'total_price_currency',\n",
    "    regexp_extract(col('total_price'), r'([A-Z]+)', 1)\n",
    ")\n",
    "\n",
    "# Join the exchange rates data frame with the reservations data frame\n",
    "exchange_rates_df = tables_dict['eur_exchange_rates']\n",
    "\n",
    "reservations_df = reservations_df.join(\n",
    "    exchange_rates_df, \n",
    "    reservations_df['total_price_currency'] == exchange_rates_df['currency_code'],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Convert total price amount to EUR\n",
    "reservations_df = reservations_df.withColumn(\n",
    "    'total_price_amount_in_eur',\n",
    "    when(\n",
    "        col('total_price_currency') == 'EUR',\n",
    "        col('total_price_amount')\n",
    "    ).otherwise(\n",
    "        col('total_price_amount') * col('currency_rate_to_eur')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Round the EUR amounts to the second decimal place\n",
    "reservations_df = reservations_df.withColumn(\n",
    "    'total_price_amount_in_eur',\n",
    "    round(col('total_price_amount_in_eur'), 2)\n",
    ")\n",
    "\n",
    "# Select desired reservations data frame columns\n",
    "reservations_df = reservations_df.select(\n",
    "    'reservation_id',\n",
    "    'reservation_date',\n",
    "    'reservation_time',\n",
    "    'customer_id',\n",
    "    'flight_id',\n",
    "    'hotel_id',\n",
    "    'room_type',\n",
    "    'total_price_amount',\n",
    "    'total_price_currency',\n",
    "    'total_price_amount_in_eur',\n",
    "    'payment_method',\n",
    ")\n",
    "\n",
    "# Overwrite the reservations data frame\n",
    "tables_dict['reservations'] = reservations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a63a8a-8ee3-43fe-9d12-f7b2e264f763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Flights Data Frame transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbb00f6-1116-43c5-a293-10c862ec125e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, date_format, col, split, regexp_replace\n",
    "\n",
    "\n",
    "flights_df = tables_dict['flights']\n",
    "\n",
    "# Convert the flight_departure column from milliseconds to timestamp\n",
    "flights_df = flights_df.withColumn(\n",
    "    'flight_departure',\n",
    "    from_unixtime(col('flight_departure') / 1000)\n",
    ")\n",
    "\n",
    "# Create separate columns for date and time\n",
    "flights_df = flights_df.withColumn(\n",
    "    'flight_departure_date', \n",
    "    date_format('flight_departure', 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'flight_departure_time', \n",
    "    date_format('flight_departure', 'HH:mm')\n",
    ")\n",
    "\n",
    "# Split the airport column into airport_code and airport_name\n",
    "flights_df = flights_df.withColumn(\n",
    "    'airport_code',\n",
    "    split(col('airport'), r'\\|').getItem(0)\n",
    ").withColumn(\n",
    "    'airport_name', \n",
    "    split(col('airport'), r'\\|').getItem(1)\n",
    ")\n",
    "\n",
    "# Extract the flight number\n",
    "flights_df = flights_df.withColumn(\n",
    "    'flight_number', \n",
    "    split(col('flight'), '-').getItem(0)\n",
    ")\n",
    "\n",
    "# Extract the airline name\n",
    "flights_df = flights_df.withColumn(\n",
    "    'flight_airline_name', \n",
    "    split(split(col('flight'), '-').getItem(1), r'\\(').getItem(0)\n",
    ")\n",
    "\n",
    "# Extract the plane model\n",
    "flights_df = flights_df.withColumn(\n",
    "    'flight_plane_model', \n",
    "    split(split(col('flight'), '-').getItem(1), r'\\(').getItem(1)\n",
    ").withColumn(\n",
    "    'flight_plane_model', \n",
    "    regexp_replace('flight_plane_model', r'\\)$', '')\n",
    ")\n",
    "\n",
    "# Select desired flights data frame columns\n",
    "flights_df = flights_df.select(\n",
    "    'flight_id',\n",
    "    'flight_departure_date',\n",
    "    'flight_departure_time',\n",
    "    'flight_number',\n",
    "    'flight_airline_name',\n",
    "    'flight_plane_model',\n",
    "    'airport_code',\n",
    "    'airport_name',\n",
    ")\n",
    "\n",
    "# Overwrite the flights data frame\n",
    "tables_dict['flights'] = flights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f9ba95-a6d2-4ba5-94a6-79cbc5f03a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hotels Data Frame transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fc71c1-4390-4927-9500-a61edb5bac0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "hotels_df = tables_dict['hotels']\n",
    "\n",
    "# Extract hotel name\n",
    "hotels_df = hotels_df.withColumn(\n",
    "    'hotel_name', \n",
    "    split(hotels_df['hotel'], '\\*').getItem(0)\n",
    ")\n",
    "\n",
    "# Extract hotel stars\n",
    "hotels_df = hotels_df.withColumn(\n",
    "    'hotel_stars', \n",
    "    split(hotels_df['hotel'], '\\*').getItem(1)\n",
    ")\n",
    "\n",
    "# Select desired hotels data frame columns\n",
    "hotels_df = hotels_df.select(\n",
    "    'hotel_id',\n",
    "    'hotel_name',\n",
    "    'hotel_stars'\n",
    ")\n",
    "\n",
    "# Overwrite the hotels data frame\n",
    "tables_dict['hotels'] = hotels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a15fa6be-6380-48b1-9f96-3bae3ab8628b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Customers Data Frame transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1cd076-cb6d-454a-ba0c-8aa1540b6fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, split, concat_ws, to_date, date_format, current_date, datediff\n",
    "\n",
    "\n",
    "customers_df = tables_dict['customers']\n",
    "\n",
    "# Extract customer first name\n",
    "customers_df = customers_df.withColumn(\n",
    "    'customer_first_name', \n",
    "    split(customers_df['customer_name'], ' ').getItem(0)\n",
    ")\n",
    "\n",
    "# Extract customer last name\n",
    "customers_df = customers_df.withColumn(\n",
    "    'customer_last_name', \n",
    "    concat_ws(' ', split(customers_df['customer_name'], ' ').getItem(1))\n",
    ")\n",
    "\n",
    "# Map customer gender from 'M'/'F' to 'Male'/'Female'\n",
    "customers_df = customers_df.withColumn(\n",
    "    'customer_gender',\n",
    "    when(customers_df['customer_gender'] == 'M', 'Male')\n",
    "    .when(customers_df['customer_gender'] == 'F', 'Female')\n",
    ")\n",
    "\n",
    "# Convert customer_date_of_birth to date type\n",
    "customers_df = customers_df.withColumn(\n",
    "    'customer_date_of_birth', \n",
    "    to_date(customers_df['customer_date_of_birth'], 'M/d/yyyy')\n",
    ")\n",
    "\n",
    "# Calculate customer age\n",
    "customers_df = customers_df.withColumn(\n",
    "    'customer_age', \n",
    "    (datediff(current_date(), customers_df['customer_date_of_birth']) / 365.25).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Change birth date format\n",
    "customers_df = customers_df.withColumn(\n",
    "    'customer_date_of_birth', \n",
    "    date_format(customers_df['customer_date_of_birth'], 'yyyy-MM-dd')\n",
    ")\n",
    "\n",
    "# Select desired customers data frame columns\n",
    "customers_df = customers_df.select(\n",
    "    'customer_id',\n",
    "    'customer_first_name',\n",
    "    'customer_last_name',\n",
    "    'customer_gender',\n",
    "    'customer_email',\n",
    "    'customer_date_of_birth',\n",
    "    'customer_age',\n",
    ")\n",
    "\n",
    "# Overwrite the customers data frame\n",
    "tables_dict['customers'] = customers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22ac1fce-3dc9-425a-8546-65e39b102876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save Data Frames to Silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b2132a-521f-4c01-91f7-7cc1843532de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "for table_name, df in tables_dict.items():\n",
    "    df.write \\\n",
    "    .format('delta') \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('mergeSchema', 'true') \\\n",
    "    .save(f'{SILVER_LAYER_PATH}/{table_name}')\n",
    "\n",
    "    print(f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} | Table \"{table_name}\" saved to silver layer.')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze-to-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
